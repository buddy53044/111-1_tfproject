{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from functools import partial\n",
    "import cv2\n",
    "from utils.dataset import parse_fn\n",
    "from utils.losses import generator_loss, discriminator_loss, gradient_penalty\n",
    "from utils.models import Generator, Discriminator\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "dataset = 'celeb_a'     # 'cifar10', 'fashion_mnist', 'mnist'\n",
    "log_dirs = 'logs_wgan_2'\n",
    "batch_size = 64\n",
    "# learning rate\n",
    "lr = 0.0001\n",
    "# Random vector size\n",
    "z_dim = 128\n",
    "# Critic updates per generator update\n",
    "n_dis = 5\n",
    "# Gradient penalty weight\n",
    "gradient_penalty_weight = 10.0\n",
    "\n",
    "\n",
    "# Load datasets and setting\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE  # 自動調整模式\n",
    "train_data, info = tfds.load(dataset, split='train+validation+test', with_info=True)\n",
    "train_data = train_data.shuffle(1000)\n",
    "train_data = train_data.map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "train_data = train_data.batch(batch_size, drop_remainder=True)      # 如果最後一批資料小於batch_size，則捨棄該批資料\n",
    "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Create networks\n",
    "generator = Generator((1, 1, z_dim))\n",
    "discriminator = Discriminator((64, 64, 3))\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "\n",
    "# Create optimizers\n",
    "g_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5, beta_2=0.9)\n",
    "d_optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_generator():\n",
    "    with tf.GradientTape() as tape:\n",
    "        # sample data\n",
    "        random_vector = tf.random.normal(shape=(batch_size, 1, 1, z_dim))\n",
    "        # create image\n",
    "        fake_img = generator(random_vector, training=True)\n",
    "        # predict real or fake\n",
    "        fake_logit = discriminator(fake_img, training=True)\n",
    "        # calculate generator loss\n",
    "        g_loss = generator_loss(fake_logit)\n",
    "\n",
    "    gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "    return g_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(real_img):\n",
    "    with tf.GradientTape() as t:\n",
    "        z = tf.random.normal(shape=(batch_size, 1, 1, z_dim))\n",
    "        fake_img = generator(z, training=True)\n",
    "\n",
    "        real_logit = discriminator(real_img, training=True)\n",
    "        fake_logit = discriminator(fake_img, training=True)\n",
    "\n",
    "        real_loss, fake_loss = discriminator_loss(real_logit, fake_logit)\n",
    "        gp = gradient_penalty(partial(discriminator, training=True), real_img, fake_img)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) + gp * gradient_penalty_weight\n",
    "\n",
    "    D_grad = t.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(D_grad, discriminator.trainable_variables))\n",
    "\n",
    "    return real_loss + fake_loss, gp\n",
    "\n",
    "\n",
    "def combine_images(images, col=10, row=10):\n",
    "    images = (images + 1) / 2\n",
    "    images = images.numpy()\n",
    "    b, h, w, _ = images.shape\n",
    "    images_combine = np.zeros(shape=(h*col, w*row, 3))\n",
    "    for y in range(col):\n",
    "        for x in range(row):\n",
    "            images_combine[y*h:(y+1)*h, x*w:(x+1)*w] = images[x+y*row]\n",
    "    return images_combine\n",
    "\n",
    "\n",
    "def train_wgan():\n",
    "    # Create tensorboard logs\n",
    "    model_dir = log_dirs + '/models/'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    summary_writer = tf.summary.create_file_writer(log_dirs)\n",
    "\n",
    "    # Create fixed Random vector for sampling\n",
    "    sample_random_vector = tf.random.normal((100, 1, 1, z_dim))\n",
    "    for epoch in range(25):\n",
    "        for step, real_img in enumerate(train_data):\n",
    "            # training discriminator\n",
    "            d_loss, gp = train_discriminator(real_img)\n",
    "            # save discriminator loss\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('discriminator_loss', d_loss, d_optimizer.iterations)\n",
    "                tf.summary.scalar('gradient_penalty', gp, d_optimizer.iterations)\n",
    "\n",
    "            # training generator\n",
    "            if d_optimizer.iterations.numpy() % n_dis == 0:\n",
    "                g_loss = train_generator()\n",
    "                # save generator loss\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('generator_loss', g_loss, g_optimizer.iterations)\n",
    "                print('G Loss: {:.2f}\\tD loss: {:.2f}\\tGP Loss {:.2f}'.format(g_loss, d_loss, gp))\n",
    "\n",
    "                # save sample\n",
    "                if g_optimizer.iterations.numpy() % 100 == 0:\n",
    "                    x_fake = generator(sample_random_vector, training=False)\n",
    "                    save_img = combine_images(x_fake)\n",
    "                    # save fake images\n",
    "                    with summary_writer.as_default():\n",
    "                        tf.summary.image(dataset, [save_img], step=g_optimizer.iterations)\n",
    "\n",
    "        # save model\n",
    "        if epoch != 0:\n",
    "            generator.save_weights(model_dir + \"generator-epochs-{}.h5\".format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_wgan()\n",
    "    # for epoch in range(1):\n",
    "    #     print('Start of epoch {}'.format(epoch))\n",
    "    #     for step, real_img in enumerate(train_data):\n",
    "    #         print(real_img.shape)\n",
    "    #         save = (real_img.numpy()[0] + 1) * 127.5\n",
    "    #         cv2.imwrite('test.png', save[..., ::-1])\n",
    "    #         assert real_img.shape[0] == 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59e28edaca8d48efe3ba8b1a9cfaf5b90f86910db2db3c89dd8dac6b042bae31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
